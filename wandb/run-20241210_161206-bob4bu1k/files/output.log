Training epoch 0/80:   0%|                                                                                        | 0/10240 [00:18<?, ?it/s]
torch.Size([4, 1, 192, 192]) torch.Size([4, 1, 192, 192])
tensor([[97.0835],
        [97.2568],
        [97.3863],
        [98.2551]], device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.4853, device='cuda:0',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
Traceback (most recent call last):
  File "/home/lbaiardi/fast-sr-unet/train_vmaf.py", line 145, in <module>
    loss_gen.backward()
  File "/home/lbaiardi/fast-sr-unet/.env/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home/lbaiardi/fast-sr-unet/.env/lib/python3.10/site-packages/torch/autograd/__init__.py", line 340, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/lbaiardi/fast-sr-unet/.env/lib/python3.10/site-packages/torch/autograd/__init__.py", line 198, in _make_grads
    raise RuntimeError(
RuntimeError: grad can be implicitly created only for scalar outputs
