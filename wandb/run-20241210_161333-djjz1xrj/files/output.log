Training epoch 0/80:   0%|                                                                                        | 0/10240 [01:45<?, ?it/s]
tensor([[96.8664],
        [96.9101],
        [96.8582],
        [96.6446]], device='cuda:0', grad_fn=<CompiledFunctionBackward>)
tensor(1.0656, device='cuda:0',
       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)
Traceback (most recent call last):
  File "/home/lbaiardi/fast-sr-unet/train_vmaf.py", line 144, in <module>
    loss_gen.backward()
  File "/home/lbaiardi/fast-sr-unet/.env/lib/python3.10/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home/lbaiardi/fast-sr-unet/.env/lib/python3.10/site-packages/torch/autograd/__init__.py", line 340, in backward
    grad_tensors_ = _make_grads(tensors, grad_tensors_, is_grads_batched=False)
  File "/home/lbaiardi/fast-sr-unet/.env/lib/python3.10/site-packages/torch/autograd/__init__.py", line 198, in _make_grads
    raise RuntimeError(
RuntimeError: grad can be implicitly created only for scalar outputs
